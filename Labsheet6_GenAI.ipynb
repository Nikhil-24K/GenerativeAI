{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-24K/GenerativeAI/blob/main/Labsheet6_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks (RNNs) for Sequence Generation : Acquire hands-on experience in constructing and training RNNs to\n",
        "        generate text sequences that are coherent and contextually relevant."
      ],
      "metadata": {
        "id": "UsnATRoqN4eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "3Gsx1OZdTunK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Shakespeare dataset"
      ],
      "metadata": {
        "id": "q04-wijyX9fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETvZeBpzTvDp",
        "outputId": "303e6fc7-7bb3-4c0d-f620-00c9b72ed7b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyqLT2gJTxQ1",
        "outputId": "b2eb9e49-29d3-4de6-d659-e5dffeae2ebe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the text"
      ],
      "metadata": {
        "id": "ebIF_aTfYUPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print(chars)\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "ids = ids_from_chars(chars)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZA4ypdST36k",
        "outputId": "3f7958a6-7e53-4f09-abbc-7e0fd051b834"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
            "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_J9yVeKWlN0",
        "outputId": "2fca0de4-ef4c-49be-e07d-c55f5db36789"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBU1RewiWwbu",
        "outputId": "403e0545-af82-4ef7-918f-9f4b10cc9aaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jm9B5w5W3sl",
        "outputId": "fa4cbc46-ca6f-4614-b722-3a3804433ffc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83RfP7PAXKig",
        "outputId": "04153bbc-c0bb-41d7-8231-bfd7a8e0a69f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyCkKxZIXO4J",
        "outputId": "76e582c1-56cd-482c-aea5-629dba74f828"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymg0ufszXTlA",
        "outputId": "a6372ea8-c2e4-46c3-9ff2-f8ec525f3cd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "wDhzmpPnXU6s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Model"
      ],
      "metadata": {
        "id": "unACdTz-ZCUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "M3RTRh7yXXKz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "b8yCdAd9XaNk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMxw5YPoXcEf",
        "outputId": "02f823d3-6334-42d4-96ed-3e7f59fe34f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "bLTGVd53XfDG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAgJ3lWdXiAB",
        "outputId": "ec0ddd82-c94b-4e12-9cf1-80cb7a1cc049"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21, 59, 51, 18, 25,  7, 30,  6, 20, 35, 35, 11, 36, 50,  7, 43, 14,\n",
              "       59, 12, 38, 35, 47, 62, 30, 37, 42, 33, 39, 41, 65, 10, 24, 27, 44,\n",
              "       16, 20, 64,  8, 10,  6, 45, 63, 49, 23, 65,  2, 54, 29, 38, 55, 11,\n",
              "       56, 54, 56, 27, 29, 16, 57,  4, 56, 52, 34,  0,  9, 49, 63, 39, 64,\n",
              "       58, 54, 48, 26, 58, 29,  0, 23,  1, 52, 64, 55, 31, 50, 19, 11, 51,\n",
              "       56, 47, 23,  6, 10, 64,  1, 35, 41, 35, 18, 16, 64, 17, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qmGtPzeXjEO",
        "outputId": "83367320-2513-4469-8cbc-76ceb1f35dc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'the painful service,\\nThe extreme dangers and the drops of blood\\nShed for my thankless country are re'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"HtlEL,Q'GVV:Wk,dAt;YVhwQXcTZbz3KNeCGy-3'fxjJz oPYp:qoqNPCr$qmU[UNK].jxZysoiMsP[UNK]J\\nmypRkF:lqhJ'3y\\nVbVECyDw\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)\n",
        "\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKPXJFe8XkYh",
        "outputId": "34ba8efe-6dab-48c5-fce4-424ce64dbb25"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1896, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.99639"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "2rHGacgzXnol"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "rJrVIS5lXpPv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "CdQS__0xZm0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGEzvc6rXqSu",
        "outputId": "d09cfc7d-cbe7-45bc-91c3-e8f99f365a13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 13s 54ms/step - loss: 2.6903\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.9698\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.6904\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.5332\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.4376\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3705\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.3193\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2748\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.2325\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1938\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1530\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1110\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0676\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0199\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9711\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.9189\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 0.8673\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.8147\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.7638\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.7174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate Next step of given input"
      ],
      "metadata": {
        "id": "tCMRWdS-lkjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "6uqZ9wqDXsLx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "ee4ko4clXuf1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass a token to generate next step"
      ],
      "metadata": {
        "id": "W-mBrusllnTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVWbZmMeXwGZ",
        "outputId": "d91ebb54-911a-41e6-fd48-904e9d8e1896"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "And I to judgment! sweet sorrow, go play the world\n",
            "Where'er I wanton find the downfall;\n",
            "and, thire I follow'd; I will give him in the towns\n",
            "That they would serve: why, no, now with stop from fell,\n",
            "What most persute change, to make up, as this trunk a\n",
            "conquer of guilty of thy deadhs,\n",
            "And incoups the heavens inclinent store or in\n",
            "That rouch'd up that love here? this stome iss, luck,\n",
            "To learn to Hind Histom Shalls up: they are\n",
            "returned sovereignty and tender haught\n",
            "Is all desires access to yield the traitor,\n",
            "And with it joyfully restoned, and good words\n",
            "To gazn disposition with us.\n",
            "\n",
            "RIVERS:\n",
            "Than done walk; King Edward's well-deserving sort,\n",
            "With some stal our wits, which else of you and him\n",
            "Are backing of the duke's uncontent. To your opinion shrew.\n",
            "\n",
            "LUCIO:\n",
            "This chief, dress'd; and therefore look not that\n",
            "Which please him like the flesh perfection which he hath\n",
            "Was in the prince that e'er I weighing. Wear, thy shreads\n",
            "standing be every day in dead became a bay of liber.\n",
            "\n",
            "HENRY BOLINGBROK \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.658386468887329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passs multiple tokens to generate sequence/story"
      ],
      "metadata": {
        "id": "mFGQrMrflu8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zZROQIyXxB0",
        "outputId": "ac189b6c-08c1-4d2b-dd80-973eb3db2ac5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nOne word with you!\\n\\nPRINCE EDWARD:\\nMy neck up my throne, to bear me as I due:\\nit. Why I should marvel our more delivery.\\n\\nKING LEWIS XI:\\nWept thou atter'd Placketa' bring forth: then thou dost not stay.\\nTo every threatening state.\\n\\nCAPULET:\\nWell, if you wear so, but I came lay, I could de\\nthere. Veep then partly and in obscureign;\\nAnd hung pauty-ment, being that which is not made to speak.\\nI tell thee, mark me, and that is not give;\\nAnd I for their best young man, for one another.\\n\\nThird Gentleman:\\nThou art the more.\\nI'll peace that seest is not Romeo, and thy bright-oratin,\\nAnd suckness and rights do marm'd a duck,\\nThat this such coznand with tears from him:\\nThe earth, good Captain Take that die melanch,\\nMy name hath made his friends that in misned pile\\nOf night shall lose the body of my sail.\\n\\nGLOUCESTER:\\nGood my lord, to Bianca fortune.\\n\\nDUKE VINCENTIO:\\nIt is to live but sost good father;\\nHowe'er this spoil to have his water? We'll tept them first wonderh\\nTheir coins shall the Lord\"\n",
            " b\"ROMEO:\\nI must believe you, sir.\\n\\nESCALUS:\\nWhy, venche out with me upon the peace:\\nCurst thy mind the ship sickly lead.\\n\\nRICHARD:\\nI cannot note it.\\n\\nCAMILLO:\\nGo to.\\nYou ure left straight\\nThat Angelo?\\n\\nESCALUS:\\nDive, but hath possiped how these two mayst thou in\\nmy sight, denied the banks of nature, go\\nTilliag it is full almost: for this letters be mother!\\nWhat, rich shall go before.\\n\\nCORIOLANUS:\\nAt what stay shall plead for thee\\nI can tell what I have to laud as but wea,\\nThat virtuous viands with slave and be,--\\nYou do me shape: we do beseech you\\nOf Phambred back again, brings forth thy face,\\nThat murder her to be so obsequict\\nThrew daughters: as she as we still, when\\nIs that she may perceive the lips--\\nShall rue the morning countermaid,\\nThat calish'd be their tears.\\n\\nCAMILLO:\\nIt more contents again.\\n\\nPETRICHAR:\\nI do me this good Verving living.\\n\\nThird Messenger:\\nThe compass is fair, and by an idle please.\\nWhy, cousin, what are thou ashore?\\n\\nKING RICHARD II:\\nMy breast may fly to him, I have w\"\n",
            " b\"ROMEO:\\nO, tends no venom, for eebty beats'd off.\\nIf not, but not my dearest, Pompey, you know my loyalty,\\nShe mock'd, as prisonous. My noble looks and thyself,\\nArt they that your wisdom shows our place,\\nOr come but use within me.\\n\\nNurse:\\nAnon, farewell.\\n\\nGREEN:\\nIt shall not warm it thus the wall.\\n\\nMARIANA:\\nAnd frowns, mine words,\\nI fear me with the ground; and both joy\\nMarch smooth the gallant when we born his captive\\nshould offend noble prisoners.\\n\\nISABELLA:\\nI am good friend;\\nFor Groun, is not she will crave thee with these,\\nwhich you are partaised, he way still be the worth\\nWhom I have pardon'd, it stranged.\\n\\nDERBY:\\nCourageous friends thee, pardon, all not bearing\\nwith child, our purpose is the house.\\n\\nPROSPERO:\\nTalk not the head is what I am, not a month\\nOf comering of the yiked rogues, and pray to\\npretting to his father's banish'd.\\n\\nMENENIUS:\\nIf I be damned Catesby! To hite five,\\nAnd by the sword to happy things:\\nUnless go fitter joies and thrusts call face,\\nWere missapent, and wherein k\"\n",
            " b\"ROMEO:\\nShe will not come.\\n\\nProvost:\\nThis was sleep and needful for sounding: but, I\\nunderstand you for your vass'd with\\nthe kingdom of primites. Thou ridgles nor many more.\\n\\nFRIAR LAURENCE:\\nHold, to be your comfort with Palis't Thanks:\\nJove of that skins, being sudden, do not know,\\nKindle hed the jealousain of the two exact,\\nAs one that ne'er would not change within the watery\\nThat I have been.\\n\\nQUEEN ELIZABETH:\\nAlack the queen with great sirs or perpetual\\nBuckingham? why fondly desire?\\n\\nABHORSON:\\nAy, by this tribl, and will. My cape he here, so both not\\nbreak, our gentrement transports by advanced:\\nAnd who let them depart? The wounds\\nOr the recorded body of his grief?\\nOur truth, and tell me straight\\nThat in eternal drum.\\n\\nSAMPSON:\\nA villain gentlemen: I will go see\\nThat heirs revenge their souls that else would quench.\\n\\nQUEEN ELIZABETH:\\nShall bring thee out of such a vanit, I'll watch you\\nbawdy; yet were it in remembrance wrange\\nHis bosom escape your honour with thy hands?\\nOr, being that wo\"\n",
            " b\"ROMEO:\\nThat's some strange;\\nTo be munitied cousin, buints amends:\\nThe mortal man not, for it should scope the ground\\nIn humour. O, let me life,\\nStand not to come in partial servant was I to die.\\n\\nFRIAR THOMAS:\\nAt my desert say to-morrow.\\n\\nDUKE OF YORK:\\nI thank my lord: what obedient traitors?\\n\\nFirst Murderer:\\nWe wash the fire, some persons have goes\\nWarwick is: and then expect make forest,\\nThus late to mine eyes,--gave thee not thy boy;\\nReigning betwixt thee to your brother:\\nAnd, bound, and pupiles withdrawing fellow.\\nHeaven leanny to question, you are the\\nFront accomply sounds all scored in the horn be made,\\nOr else new father's son, who does be doubted from\\nThe instrument, which serves my sun is strange;\\nAnd from this marriage deceiting haught again\\nThan that wilthes drunk for England and\\nOur fire with back against me,\\nAnd shuts for thy dear better than my love's care\\nThat was his title out.\\n\\nDUKE VINCENTIO:\\nVelp thee, King Henry's corn:\\nAnd who revenged homely bed her beauty?\\nWhat must I \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.107358455657959\n"
          ]
        }
      ]
    }
  ]
}